{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is to explore the usage of datasets in HF, primarily by following the [official tutorial](https://huggingface.co/docs/datasets/tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "You can read metadata using `load_dataset_builder` before committing to downloading the dataset.  Load the dataset using `load_dataset`, specifying split if desired.  Splits can be found using `get_dataset_split_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcnewcp/code/contextual-qa-chat-app/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review Dataset.\n",
      "This is a dataset of containing 5,331 positive and 5,331 negative processed\n",
      "sentences from Rotten Tomatoes movie reviews. This data was first used in Bo\n",
      "Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for\n",
      "sentiment categorization with respect to rating scales.'', Proceedings of the\n",
      "ACL, 2005.\n",
      "\n",
      "Features \n",
      " {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "dataset_name = \"rotten_tomatoes\"\n",
    "\n",
    "ds_builder = load_dataset_builder(dataset_name)\n",
    "print(ds_builder.info.description)\n",
    "print(\"Features \\n\", ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits:  ['train', 'validation', 'test']\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "\n",
    "print(\"Splits: \", get_dataset_split_names(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "You can index the dataset by row with integers, or by column with column name.  You can also use ranges of values to slice the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
       " \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\",\n",
       " 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3:6][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "It is also possible to perform preprocessing tasks on the `dataset` object, including:\n",
    "- tokenize text data\n",
    "- resample audio data\n",
    "- transform or augment image data\n",
    "\n",
    "Most relevant to this project is tokenization so I'll explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 32.6kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 1.33MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 9.45MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 8.98MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 11276.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format\n",
    "You can also name fields and define data formats to work with particular modeling frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "PyTorch needs to be installed to be able to return PyTorch tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\u001b[39m.\u001b[39;49mset_format(\u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtorch\u001b[39;49m\u001b[39m\"\u001b[39;49m, columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m dataset\u001b[39m.\u001b[39mformat[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/code/contextual-qa-chat-app/.venv/lib/python3.11/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/code/contextual-qa-chat-app/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2525\u001b[0m, in \u001b[0;36mDataset.set_format\u001b[0;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[1;32m   2523\u001b[0m \u001b[39m# Check that the format_type and format_kwargs are valid and make it possible to have a Formatter\u001b[39;00m\n\u001b[1;32m   2524\u001b[0m \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m get_format_type_from_alias(\u001b[39mtype\u001b[39m)\n\u001b[0;32m-> 2525\u001b[0m get_formatter(\u001b[39mtype\u001b[39;49m, features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info\u001b[39m.\u001b[39;49mfeatures, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mformat_kwargs)\n\u001b[1;32m   2527\u001b[0m \u001b[39m# Check filter column\u001b[39;00m\n\u001b[1;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(columns, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/code/contextual-qa-chat-app/.venv/lib/python3.11/site-packages/datasets/formatting/__init__.py:128\u001b[0m, in \u001b[0;36mget_formatter\u001b[0;34m(format_type, **format_kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m _FORMAT_TYPES[format_type](\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m format_type \u001b[39min\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE:\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    131\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReturn type should be None or selected in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mtype\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtype\u001b[39m\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m_FORMAT_TYPES\u001b[39m.\u001b[39mkeys()\u001b[39m \u001b[39m\u001b[39mif\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtype\u001b[39m\u001b[39m \u001b[39m\u001b[39m!=\u001b[39m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mformat_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: PyTorch needs to be installed to be able to return PyTorch tensors."
     ]
    }
   ],
   "source": [
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "dataset.format['type']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
